{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f307c881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "#  PyG: Train & Evaluate GCN on Cora Dataset\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2ffdf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ------------------- 1. Load Dataset ---------------------------\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0]  # Single graph\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda:4' if torch.cuda.is_available() else 'cpu')\n",
    "data = data.to(device)\n",
    "\n",
    "# ------------------- 2. Define GCN Model -----------------------\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x  # raw logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "119076d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 2708])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def edge_index_to_dense_adj(edge_index, num_nodes=None, include_self_loops=True):\n",
    "    if num_nodes is None:\n",
    "        num_nodes = int(edge_index.max()) + 1\n",
    "    \n",
    "    # 初始化全零稠密矩阵\n",
    "    adj = torch.zeros(num_nodes, num_nodes, dtype=torch.float, device=edge_index.device)\n",
    "    \n",
    "    # 填充边（自动去重 + 累加）\n",
    "    row, col = edge_index\n",
    "    adj[row, col] = 1.0\n",
    "    \n",
    "    # 无向图 → 对称化\n",
    "    adj = adj + adj.t()  # 现在每条无向边权重为 2（可除以 2 归一化）\n",
    "    adj = (adj > 0).float()  # 转为 0/1 邻接矩阵\n",
    "    \n",
    "    # （可选）加入自环\n",
    "    if include_self_loops:\n",
    "        adj.fill_diagonal_(1.0)\n",
    "    \n",
    "    return adj\n",
    "\n",
    "# 使用示例\n",
    "adj_dense = edge_index_to_dense_adj(data.edge_index, num_nodes=data.num_nodes)\n",
    "adj_dense.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7774d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 1.]], device='cuda:4')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28a6f0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_channels, hidden_channels)\n",
    "        self.lin2 = torch.nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return x  # raw logits\n",
    "    \n",
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin(x)\n",
    "        return x  # raw logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54ba249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=8, out_channels=None, dropout=0.6):\n",
    "        super().__init__()\n",
    "        # Layer 1: 8 heads × 8 feats/head → 64 total\n",
    "        self.conv1 = GATConv(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=hidden_channels,\n",
    "            heads=8,\n",
    "            concat=True,        # default, merge heads\n",
    "            dropout=dropout,\n",
    "            add_self_loops=True,\n",
    "            bias=True\n",
    "        )\n",
    "        # Layer 2: 1 head for classification\n",
    "        self.conv2 = GATConv(\n",
    "            in_channels=hidden_channels * 8,\n",
    "            out_channels=out_channels,\n",
    "            heads=1,\n",
    "            concat=False,       # average heads → scalar per class\n",
    "            dropout=dropout,\n",
    "            add_self_loops=True,\n",
    "            bias=True\n",
    "        )\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # ---- Layer 1 ----\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "\n",
    "        # ---- Layer 2 ----\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return x  # logits (for NLLLoss after log_softmax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b934deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGATLayerMultiHead(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_head):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "        self.leaky_relu = torch.nn.LeakyReLU()\n",
    "        self.num_head = num_head\n",
    "        \n",
    "\n",
    "    def forward(self, H, A):\n",
    "        # n, d -> h, n, d_h\n",
    "        n, d = H.shape[0], self.output_dim\n",
    "        h = self.num_head\n",
    "        H = self.linear(H)\n",
    "\n",
    "        H = H.view(n, h, d//h)\n",
    "        H = H.transpose(0,1) # h, n, d_h\n",
    "        Att = H @ H.transpose(1,2) # h, n, n\n",
    "        Att = F.leaky_relu(Att) \n",
    "        # Att = Att * A # 这个mask 有问题, 会导致边泄漏\n",
    "        Att = Att.masked_fill(A==0, -torch.inf)\n",
    "        Att = F.softmax(Att, dim=1)\n",
    "        H = Att @ H # h, n, d_h\n",
    "        H = H.transpose(0,1)\n",
    "        H = H.contiguous().view(n,d)\n",
    "        return H\n",
    "\n",
    "class MyGATMultiHead(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_head=8):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.gat_layer1 = MyGATLayerMultiHead(input_dim, hidden_dim, num_head)\n",
    "        self.gat_layer2 = MyGATLayerMultiHead(hidden_dim, hidden_dim, num_head)\n",
    "        self.projector = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.dropout = torch.nn.Dropout(p=0.6)\n",
    "\n",
    "    def forward(self, H, A):\n",
    "        H = self.dropout(H)\n",
    "        H = self.gat_layer1(H, A)\n",
    "        H = self.elu(H)\n",
    "\n",
    "        H = self.dropout(H)\n",
    "        H = self.gat_layer2(H, A)\n",
    "        H = self.elu(H)\n",
    "\n",
    "        H = self.projector(H)\n",
    "\n",
    "        return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "969e1d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGATLayer(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "        self.leaky_relu = torch.nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, H, A):\n",
    "        H = self.linear(H)\n",
    "        Att = H @ H.T\n",
    "        Att = self.leaky_relu(Att)\n",
    "        # Att = Att * A # 这个mask 有问题, 会导致边泄漏\n",
    "        Att = Att.masked_fill(A==0, -torch.inf)\n",
    "        Att = F.softmax(Att, dim=1)\n",
    "        H = Att @ H\n",
    "        return H\n",
    "\n",
    "class MyGAT(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.gat_layer1 = MyGATLayer(input_dim, hidden_dim)\n",
    "        self.gat_layer2 = MyGATLayer(hidden_dim, output_dim)\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.dropout = torch.nn.Dropout(p=0.6)\n",
    "\n",
    "    def forward(self, H, A):\n",
    "        H = self.dropout(H)\n",
    "        H = self.gat_layer1(H, A)\n",
    "        H = self.elu(H)\n",
    "\n",
    "        H = self.dropout(H)\n",
    "        H = self.gat_layer2(H, A)\n",
    "        return H\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "60d5cebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch: 001, Loss: 1.9464, Val Acc: 0.1580\n",
      "Epoch: 010, Loss: 1.9150, Val Acc: 0.3760\n",
      "Epoch: 020, Loss: 1.8860, Val Acc: 0.5760\n",
      "Epoch: 030, Loss: 1.8390, Val Acc: 0.6660\n",
      "Epoch: 040, Loss: 1.8097, Val Acc: 0.7080\n",
      "Epoch: 050, Loss: 1.7621, Val Acc: 0.7440\n",
      "Epoch: 060, Loss: 1.7195, Val Acc: 0.7620\n",
      "Epoch: 070, Loss: 1.6851, Val Acc: 0.7740\n",
      "Epoch: 080, Loss: 1.6134, Val Acc: 0.7720\n",
      "Epoch: 090, Loss: 1.5640, Val Acc: 0.7760\n",
      "Epoch: 100, Loss: 1.4756, Val Acc: 0.7800\n",
      "Epoch: 110, Loss: 1.4317, Val Acc: 0.7840\n",
      "Epoch: 120, Loss: 1.3514, Val Acc: 0.7840\n",
      "Epoch: 130, Loss: 1.2166, Val Acc: 0.7800\n",
      "Epoch: 140, Loss: 1.1521, Val Acc: 0.7800\n",
      "Epoch: 150, Loss: 1.0278, Val Acc: 0.7800\n",
      "Epoch: 160, Loss: 1.0162, Val Acc: 0.7760\n",
      "Epoch: 170, Loss: 0.9658, Val Acc: 0.7800\n",
      "Epoch: 180, Loss: 0.8103, Val Acc: 0.7800\n",
      "Epoch: 190, Loss: 0.7730, Val Acc: 0.7800\n",
      "Epoch: 200, Loss: 0.7173, Val Acc: 0.7740\n",
      "\n",
      "Final Test Accuracy: 0.7880\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Hyperparameters\n",
    "# hidden_channels = 16\n",
    "# learning_rate = 0.01\n",
    "# weight_decay = 5e-4\n",
    "epochs = 200\n",
    "\n",
    "# model = GCN(\n",
    "#     in_channels=dataset.num_node_features,\n",
    "#     hidden_channels=hidden_channels,\n",
    "#     out_channels=dataset.num_classes\n",
    "# ).to(device)\n",
    "\n",
    "\n",
    "# model = MLP(\n",
    "#     in_channels=dataset.num_node_features,\n",
    "#     hidden_channels=hidden_channels,\n",
    "#     out_channels=dataset.num_classes\n",
    "# ).to(device)\n",
    "\n",
    "# model = LinearModel(\n",
    "#     in_channels=dataset.num_node_features,\n",
    "#     out_channels=dataset.num_classes\n",
    "# ).to(device)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "# ------------------- 3. Hyperparameters ------------------------\n",
    "# model = GAT(\n",
    "#     in_channels=dataset.num_node_features,\n",
    "#     hidden_channels=8,                    # F' = 8\n",
    "#     out_channels=dataset.num_classes,\n",
    "#     dropout=0.6\n",
    "# ).to(device)\n",
    "\n",
    "model = MyGAT(\n",
    "    input_dim=dataset.num_node_features,\n",
    "    hidden_dim=256,                    # F' = 8\n",
    "    output_dim=dataset.num_classes,\n",
    ").to(device)\n",
    "\n",
    "# model = MyGATMultiHead(\n",
    "#     input_dim=dataset.num_node_features,\n",
    "#     hidden_dim=256,                    # F' = 8\n",
    "#     output_dim=dataset.num_classes,\n",
    "#     num_head = 8\n",
    "# ).to(device)\n",
    "\n",
    "# L2 regularization strength\n",
    "weight_decay = 0.0005  # λ = 5e-4 for Cora & Citeseer\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=weight_decay)\n",
    "\n",
    "Adj = edge_index_to_dense_adj(data.edge_index, num_nodes=data.num_nodes)\n",
    "\n",
    "# ------------------- 3. Training & Eval Functions -------------\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # out = model(data.x, data.edge_index)\n",
    "    out = model(data.x, Adj)\n",
    "    # out = model(data.x)\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(mask):\n",
    "    model.eval()\n",
    "    logits = model(data.x, Adj)\n",
    "    # logits = model(data.x)\n",
    "    pred = logits[mask].max(1)[1]\n",
    "    acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "    return acc\n",
    "\n",
    "# ------------------- 4. Training Loop -------------------------\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train()\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        val_acc = evaluate(data.val_mask)\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "# ------------------- 5. Final Test Evaluation -----------------\n",
    "test_acc = evaluate(data.test_mask)\n",
    "print(f'\\nFinal Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b0afd0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1433"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a117522c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.5847, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 7.8410, 1.5073,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.5073, 6.6483,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 6.3402, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 5.0924, 1.0883],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0883, 4.9292]],\n",
      "       device='cuda:4', grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000], device='cuda:4',\n",
      "       grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "H = data.x\n",
    "n, d = H.shape\n",
    "linear = torch.nn.Linear(d, d).to(device)\n",
    "\n",
    "\n",
    "H = linear(H)\n",
    "Att = H @ H.T\n",
    "Att = F.leaky_relu(Att)\n",
    "Att = Att * Adj\n",
    "print(Att)\n",
    "Att = F.softmax(Att, dim=1)\n",
    "print(Att.sum(1))\n",
    "H = Att @ H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ad3f1f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([1.], requires_grad=True)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Parameter(torch.ones(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "87a8e5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0055,  0.0516,  0.0024,  ..., -0.0196,  0.1127,  0.0521],\n",
       "        [-0.0165,  0.1021,  0.0360,  ...,  0.0028,  0.0469, -0.0018],\n",
       "        [ 0.0524,  0.0039,  0.0172,  ...,  0.0198, -0.0031, -0.0038],\n",
       "        ...,\n",
       "        [-0.0537,  0.0097,  0.0401,  ...,  0.0306,  0.0290,  0.0543],\n",
       "        [ 0.0275,  0.0190, -0.0480,  ...,  0.0496,  0.0347,  0.0139],\n",
       "        [ 0.0166, -0.0029, -0.0681,  ..., -0.0406,  0.0346,  0.0456]],\n",
       "       device='cuda:4', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = data.x\n",
    "linear = torch.nn.Linear(H.shape[1], 128).to(device)\n",
    "\n",
    "\n",
    "n, d = H.shape[0], 128\n",
    "h = 8\n",
    "H = linear(H)\n",
    "H = H.view(n, h, d//h)\n",
    "H = H.transpose(0,1) # h, n, d_h\n",
    "Att = H @ H.transpose(1,2) # h, n, n\n",
    "Att = F.leaky_relu(Att) \n",
    "# Att = Att * A # 这个mask 有问题, 会导致边泄漏\n",
    "Att = Att.masked_fill(Adj==0, -torch.inf)\n",
    "Att = F.softmax(Att, dim=1)\n",
    "H = Att @ H # h, n, d_h\n",
    "H = H.transpose(0,1)\n",
    "H = H.contiguous().view(n,d)\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c791d60",
   "metadata": {},
   "source": [
    "RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e5f7b1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 1433])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "567e9b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1433]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in range(1,1434) if 1433 % i == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6914821e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
